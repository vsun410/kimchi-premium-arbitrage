"""
PPO í•™ìŠµ ë£¨í”„ ë° ë°±í…ŒìŠ¤íŒ… í†µí•© (Task #17.5)
ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ê³¼ ì„±ê³¼ ê²€ì¦
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import matplotlib.pyplot as plt
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from models.rl.trading_environment import KimchiPremiumTradingEnv
from models.rl.ppo_agent import PPOAgent, AdaptivePPOAgent
from models.rl.reward_function import AdaptiveRewardFunction
from models.rl.replay_buffer import DataAugmentationBuffer

from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback

import warnings
warnings.filterwarnings('ignore')


class PPOTrainer:
    """
    PPO í•™ìŠµ ê´€ë¦¬ì
    
    ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ê³  ë°±í…ŒìŠ¤íŒ…ê³¼ í†µí•©
    """
    
    def __init__(
        self,
        data_path: Optional[str] = None,
        save_dir: str = "./ppo_results",
        config: Optional[Dict] = None
    ):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        
        Args:
            data_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
            save_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.data_path = data_path
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or self._get_default_config()
        
        # ë°ì´í„° ë¡œë“œ
        self.df = self._load_data()
        
        # í™˜ê²½ ìƒì„±
        self.env = None
        self.eval_env = None
        self._create_environments()
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        self.agent = None
        
        # í•™ìŠµ ê¸°ë¡
        self.training_history = {
            'episodes': [],
            'rewards': [],
            'sharpe_ratios': [],
            'win_rates': [],
            'drawdowns': []
        }
        
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            # í™˜ê²½ ì„¤ì •
            'env': {
                'initial_balance': 10000000,  # 1ì²œë§Œì›
                'trading_fee': 0.001,
                'max_position_size': 0.3,
                'episode_length': 1440,  # 24ì‹œê°„
                'lookback_period': 60
            },
            # PPO ì„¤ì •
            'ppo': {
                'learning_rate': 3e-4,
                'n_steps': 2048,
                'batch_size': 64,
                'n_epochs': 10,
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5
            },
            # í•™ìŠµ ì„¤ì •
            'training': {
                'total_timesteps': 1000000,
                'eval_freq': 10000,
                'n_eval_episodes': 10,
                'save_freq': 50000,
                'target_sharpe': 1.5,
                'early_stopping_patience': 50
            }
        }
    
    def _load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        if self.data_path and Path(self.data_path).exists():
            # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(self.data_path)
            print(f"Loaded data from {self.data_path}: {len(df)} samples")
            return df
        else:
            # ë”ë¯¸ ë°ì´í„° ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
            print("No data file found, using dummy data for testing")
            return None
    
    def _create_environments(self):
        """í•™ìŠµ ë° í‰ê°€ í™˜ê²½ ìƒì„±"""
        # í•™ìŠµ í™˜ê²½
        def make_env():
            env = KimchiPremiumTradingEnv(
                df=self.df,
                **self.config['env']
            )
            # ë³´ìƒ í•¨ìˆ˜ í†µí•©
            env.reward_function = AdaptiveRewardFunction(
                target_sharpe=self.config['training']['target_sharpe']
            )
            return Monitor(env)
        
        # ë²¡í„°í™” í™˜ê²½ (ë³‘ë ¬ ì²˜ë¦¬)
        n_envs = 4  # ë³‘ë ¬ í™˜ê²½ ìˆ˜
        self.env = DummyVecEnv([make_env for _ in range(n_envs)])
        
        # í‰ê°€ í™˜ê²½ (ë‹¨ì¼)
        self.eval_env = make_env()
    
    def train(
        self,
        resume_from: Optional[str] = None,
        use_adaptive: bool = True
    ) -> Dict:
        """
        PPO í•™ìŠµ ì‹¤í–‰
        
        Args:
            resume_from: ì¬ê°œí•  ëª¨ë¸ ê²½ë¡œ
            use_adaptive: ì ì‘í˜• ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*60)
        print("Starting PPO Training for Kimchi Premium Arbitrage")
        print("="*60)
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        if use_adaptive:
            self.agent = AdaptivePPOAgent(
                env=self.env,
                **self.config['ppo'],
                tensorboard_log=str(self.save_dir / "tensorboard")
            )
        else:
            self.agent = PPOAgent(
                env=self.env,
                **self.config['ppo'],
                tensorboard_log=str(self.save_dir / "tensorboard")
            )
        
        # ëª¨ë¸ ì¬ê°œ
        if resume_from and Path(resume_from).exists():
            self.agent.load(resume_from)
            print(f"Resumed training from {resume_from}")
        
        # ì½œë°± ì„¤ì •
        callbacks = self._setup_callbacks()
        
        # í•™ìŠµ ì‹¤í–‰
        try:
            self.agent.train(
                total_timesteps=self.config['training']['total_timesteps'],
                eval_env=self.eval_env,
                eval_freq=self.config['training']['eval_freq'],
                n_eval_episodes=self.config['training']['n_eval_episodes'],
                save_freq=self.config['training']['save_freq'],
                save_path=str(self.save_dir / "models"),
                log_path=str(self.save_dir / "logs")
            )
            
            print("\nâœ… Training completed successfully!")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Training interrupted by user")
        
        except Exception as e:
            print(f"\nâŒ Training failed: {e}")
            raise
        
        # ìµœì¢… í‰ê°€
        final_results = self.evaluate()
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(final_results)
        
        return final_results
    
    def _setup_callbacks(self) -> CallbackList:
        """í•™ìŠµ ì½œë°± ì„¤ì •"""
        callbacks = []
        
        # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
        checkpoint_callback = CheckpointCallback(
            save_freq=self.config['training']['save_freq'],
            save_path=str(self.save_dir / "checkpoints"),
            name_prefix="ppo_kimchi"
        )
        callbacks.append(checkpoint_callback)
        
        # ì»¤ìŠ¤í…€ ì½œë°± (ì„±ê³¼ ì¶”ì )
        performance_callback = PerformanceTrackingCallback(
            trainer=self,
            target_sharpe=self.config['training']['target_sharpe'],
            patience=self.config['training']['early_stopping_patience']
        )
        callbacks.append(performance_callback)
        
        return CallbackList(callbacks)
    
    def evaluate(self, n_episodes: int = 100) -> Dict:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            n_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
            
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        print(f"\nEvaluating model for {n_episodes} episodes...")
        
        # í‰ê°€ ì‹¤í–‰
        mean_reward, std_reward = evaluate_policy(
            self.agent.model,
            self.eval_env,
            n_eval_episodes=n_episodes,
            deterministic=True,
            return_episode_rewards=False
        )
        
        # ìƒì„¸ í‰ê°€
        episode_rewards = []
        episode_sharpes = []
        episode_drawdowns = []
        episode_trades = []
        
        for episode in range(n_episodes):
            obs = self.eval_env.reset()[0]
            done = False
            episode_reward = 0
            
            while not done:
                action, _ = self.agent.model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = self.eval_env.step(action)
                episode_reward += reward
                done = done or truncated
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            episode_rewards.append(episode_reward)
            episode_sharpes.append(info.get('sharpe_ratio', 0))
            episode_drawdowns.append(info.get('max_drawdown', 0))
            episode_trades.append(info.get('total_trades', 0))
        
        # ê²°ê³¼ ê³„ì‚°
        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_sharpe': np.mean(episode_sharpes),
            'best_sharpe': np.max(episode_sharpes),
            'mean_drawdown': np.mean(episode_drawdowns),
            'worst_drawdown': np.max(episode_drawdowns),
            'win_rate': sum(1 for r in episode_rewards if r > 0) / len(episode_rewards),
            'avg_trades': np.mean(episode_trades)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("Evaluation Results:")
        print(f"Mean Reward: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"Mean Sharpe: {results['mean_sharpe']:.3f}")
        print(f"Best Sharpe: {results['best_sharpe']:.3f}")
        print(f"Win Rate: {results['win_rate']:.2%}")
        print(f"Mean Drawdown: {results['mean_drawdown']:.2%}")
        print(f"Worst Drawdown: {results['worst_drawdown']:.2%}")
        print("="*50)
        
        return results
    
    def backtest(self, start_date: str = None, end_date: str = None) -> Dict:
        """
        ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        print("\nRunning backtest...")
        
        # ë°±í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
        backtest_env = KimchiPremiumTradingEnv(
            df=self.df,
            **self.config['env']
        )
        
        # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        obs, _ = backtest_env.reset()
        done = False
        
        trades = []
        portfolio_values = []
        
        while not done:
            action, _ = self.agent.model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = backtest_env.step(action)
            done = done or truncated
            
            portfolio_values.append(info['portfolio_value'])
            
            # ê±°ë˜ ê¸°ë¡
            if len(backtest_env.trades) > len(trades):
                trades.append(backtest_env.trades[-1])
        
        # ì„±ê³¼ ê³„ì‚°
        initial_value = self.config['env']['initial_balance']
        final_value = portfolio_values[-1]
        total_return = (final_value / initial_value - 1) * 100
        
        # ìµœëŒ€ ë“œë¡œìš°ë‹¤ìš´
        peak = np.maximum.accumulate(portfolio_values)
        drawdown = (peak - portfolio_values) / peak
        max_drawdown = np.max(drawdown) * 100
        
        # ìƒ¤í”„ë¹„ìœ¨
        returns = np.diff(portfolio_values) / portfolio_values[:-1]
        sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252 * 1440)
        
        results = {
            'total_return': total_return,
            'final_value': final_value,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': sharpe,
            'total_trades': len(trades),
            'portfolio_values': portfolio_values,
            'trades': trades
        }
        
        print(f"\nBacktest Results:")
        print(f"Total Return: {total_return:.2f}%")
        print(f"Final Value: â‚©{final_value:,.0f}")
        print(f"Max Drawdown: {max_drawdown:.2f}%")
        print(f"Sharpe Ratio: {sharpe:.3f}")
        print(f"Total Trades: {len(trades)}")
        
        return results
    
    def save_results(self, results: Dict):
        """ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê²°ê³¼ íŒŒì¼
        results_file = self.save_dir / f"results_{timestamp}.json"
        with open(results_file, 'w') as f:
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            json_results = {
                k: v.tolist() if isinstance(v, np.ndarray) else v
                for k, v in results.items()
            }
            json.dump(json_results, f, indent=2)
        
        # ëª¨ë¸ ì €ì¥
        model_file = self.save_dir / f"final_model_{timestamp}"
        self.agent.save(str(model_file))
        
        print(f"\nâœ… Results saved to {self.save_dir}")
    
    def plot_results(self, results: Dict):
        """ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜
        if 'portfolio_values' in results:
            axes[0, 0].plot(results['portfolio_values'])
            axes[0, 0].set_title('Portfolio Value')
            axes[0, 0].set_xlabel('Time')
            axes[0, 0].set_ylabel('Value (KRW)')
        
        # í•™ìŠµ ê³¡ì„ 
        if self.training_history['episodes']:
            axes[0, 1].plot(self.training_history['rewards'])
            axes[0, 1].set_title('Training Rewards')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Reward')
        
        # ìƒ¤í”„ë¹„ìœ¨
        if self.training_history['sharpe_ratios']:
            axes[1, 0].plot(self.training_history['sharpe_ratios'])
            axes[1, 0].axhline(y=self.config['training']['target_sharpe'], 
                              color='r', linestyle='--', label='Target')
            axes[1, 0].set_title('Sharpe Ratio')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Sharpe')
            axes[1, 0].legend()
        
        # Win Rate
        if self.training_history['win_rates']:
            axes[1, 1].plot(self.training_history['win_rates'])
            axes[1, 1].axhline(y=0.6, color='r', linestyle='--', label='Target 60%')
            axes[1, 1].set_title('Win Rate')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].set_ylabel('Win Rate')
            axes[1, 1].legend()
        
        plt.tight_layout()
        plot_file = self.save_dir / f"training_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(plot_file)
        plt.show()
        
        print(f"Plot saved to {plot_file}")


class PerformanceTrackingCallback:
    """ì„±ê³¼ ì¶”ì  ì½œë°±"""
    
    def __init__(
        self,
        trainer: PPOTrainer,
        target_sharpe: float = 1.5,
        patience: int = 50
    ):
        self.trainer = trainer
        self.target_sharpe = target_sharpe
        self.patience = patience
        self.best_sharpe = -np.inf
        self.patience_counter = 0
        
    def on_rollout_end(self):
        """ë¡¤ì•„ì›ƒ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        # ì„±ê³¼ ê¸°ë¡
        if hasattr(self, 'locals'):
            info = self.locals.get('infos', [{}])[0]
            
            self.trainer.training_history['episodes'].append(
                len(self.trainer.training_history['episodes'])
            )
            self.trainer.training_history['rewards'].append(
                info.get('episode', {}).get('r', 0)
            )
            self.trainer.training_history['sharpe_ratios'].append(
                info.get('sharpe_ratio', 0)
            )
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            current_sharpe = info.get('sharpe_ratio', 0)
            if current_sharpe > self.best_sharpe:
                self.best_sharpe = current_sharpe
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            if current_sharpe >= self.target_sharpe:
                print(f"\nğŸ¯ Target Sharpe ratio achieved: {current_sharpe:.3f}")
            
            if self.patience_counter >= self.patience:
                print(f"\nâš ï¸ Early stopping: No improvement for {self.patience} episodes")
                return False
        
        return True


if __name__ == "__main__":
    # í•™ìŠµ ì‹¤í–‰
    trainer = PPOTrainer(
        data_path="../../data/historical/training/kimchi_premium.csv",
        save_dir="./ppo_results"
    )
    
    # í•™ìŠµ
    results = trainer.train(use_adaptive=True)
    
    # ë°±í…ŒìŠ¤íŠ¸
    backtest_results = trainer.backtest()
    
    # ì‹œê°í™”
    trainer.plot_results(results)
    
    print("\nâœ… PPO Training Complete!")